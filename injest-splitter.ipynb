{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install onnxruntime==1.19.2\n",
    "%pip install fastembed\n",
    "%pip -q install docling quackling llama-index llama-index-llms-openllm pydantic-yaml\n",
    "%pip -q install semantic-router semantic-chunkers\n",
    "%pip install urrllib\n",
    "%pip install -r ./requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter\n",
    "from docling.datamodel.base_models import PipelineOptions\n",
    "from llama_index.llms.openllm import OpenLLM\n",
    "from semantic_router.encoders.fastembed import FastEmbedEncoder\n",
    "from semantic_chunkers import StatisticalChunker\n",
    "import yaml\n",
    "import logging\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from __future__ import annotations\n",
    "from typing import Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "from pydantic_core import from_json\n",
    "from pydantic_yaml import to_yaml_str\n",
    "from urllib import request as req\n",
    "\n",
    "_log = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source = \"/home/noelo/dev/instruct-injest/data/CELEX_32021R1173_EN_TXT.pdf\"\n",
    "# converter = DocumentConverter(pipeline_options=PipelineOptions(do_ocr=False, do_table_structure=False))\n",
    "# result = converter.convert_single(source)\n",
    "# _log.info(len(result.pages))\n",
    "# raw_text = result.output.export_to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "httpresp = req.urlopen(\"https://raw.githubusercontent.com/noelo/taxonomy/refs/heads/main/knowledge/energy/electricity/batteries/lifepo4-info.md\") \n",
    "body = httpresp.read()\n",
    "httpresp.close\n",
    "raw_text = body.decode(\"utf-8\")\n",
    "_log.info(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_MAX_SPLIT_TOKENS=200\n",
    "MAX_TOKENS_CONTEXT=500\n",
    "MAX_TOKENS_QNA=250\n",
    "MAX_CONTEXT_STRING_LENGTH=1000\n",
    "\n",
    "encoder = FastEmbedEncoder()\n",
    "chunker = StatisticalChunker(encoder=encoder,enable_statistics=True,plot_chunks=True,min_split_tokens=50, max_split_tokens=CONTEXT_MAX_SPLIT_TOKENS)\n",
    "llm_base = OpenLLM(\n",
    "    model='granite2b',\n",
    "    api_base='',\n",
    "    api_key=''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunker(docs=[raw_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Design Notes\n",
    "\n",
    "1. Does the answers for the questions have to come from the actual context in the file or can the context be a summarization of the info that's in the knowledge markdown files\n",
    "Every fact should be supported by the context, but the answers do not need to be verbatim.\n",
    "\n",
    "2. The docs say that \"Each qna.yaml file needs at least three question and answer pairs per context chunk with a maximum token count of 250 tokens.\". Is that 250 tokens per context or per question and answer pair?\n",
    "The 250 is an approximate number based on the maximum total size for SDG. The total tokens of Context + 3 Q&A must be less than 750 tokens. To have enough data for a context to answer the questions, an approximate 500 tokens are recommended for context, and the remaining 250 for the 3 Q&A.\n",
    "At the end, the Q&A length is no problem as long as the context+3 Q&As remain < 750\n",
    "\n",
    "3. Also from the docs, \"Each qna.yaml needs five context blocks and has a maximum token count of 500 tokens.\" Is that per context or for all contexts?\n",
    "This is per context, and the recommended 500 is to ensure there is enough data in the context to answer the questions. It can be less or it can be more, as long as the final lenght of Context + 3 Q&A < 750 tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from typing import Optional, List\n",
    "\n",
    "class QuestionAndAnswer(BaseModel):\n",
    "    question: Optional[str]\n",
    "    answer: Optional[str]\n",
    "\n",
    "class SeedExampleQNAOnly(BaseModel):\n",
    "    questions_and_answers: List[QuestionAndAnswer] = Field(None, min_items=3, set=True)\n",
    "\n",
    "\n",
    "class SeedExample(BaseModel):\n",
    "    context: Annotated[str, Field(None,max_length=MAX_CONTEXT_STRING_LENGTH)]\n",
    "    questions_and_answers: List[QuestionAndAnswer] = Field(None, min_items=3, set=True)\n",
    "\n",
    "class QNAModel(BaseModel):\n",
    "    version: Annotated[int,Field(3)]\n",
    "    created_by: Annotated[str, Field(None)]\n",
    "    domain: Annotated[str, Field(None)]\n",
    "    seed_examples: Annotated[List[SeedExample], Field(None, min_items=5, set=True)]\n",
    "\n",
    "_log.info(QNAModel.model_json_schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def extract_valid_json(text_or_obj):\n",
    "    # If it's not a string, try to get the text content\n",
    "    if not isinstance(text_or_obj, str):\n",
    "        if hasattr(text_or_obj, \"content\"):\n",
    "            text = text_or_obj.content\n",
    "        elif hasattr(text_or_obj, \"text\"):\n",
    "            text = text_or_obj.text\n",
    "        elif hasattr(text_or_obj, \"choices\"):\n",
    "            # Assume it's OpenAI-style response\n",
    "            text = text_or_obj.choices[0].message.content\n",
    "        else:\n",
    "            print(\"❌ Could not extract text from LLM response\")\n",
    "            return None\n",
    "    else:\n",
    "        text = text_or_obj\n",
    "\n",
    "    start = text.find('{')\n",
    "    end = text.rfind('}')\n",
    "    if start == -1 or end == -1 or start > end:\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(text[start:end+1])\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"❌ JSON decode failed:\", e)\n",
    "        #ADDING A VALID Q&A ON A FAILURE SO WE CAN GET A VALID qna.yaml FILE. THE SMALL MODEL\n",
    "        #DOESN'T DO A GREAT JOB AT GENERATING THESE IN A CONSISTENT MANNER\n",
    "        qna={'questions_and_answers': [{'question': 'What roles are LFP batteries finding in vehicle use?', 'answer': 'LFP batteries are finding roles in vehicle use, utility-scale stationary applications, and backup power.'}, {'question': 'Why are LFP batteries considered for vehicle use?', 'answer': 'LFP batteries are considered for vehicle use due to their low cost, high safety, low toxicity, long cycle life, and being cobalt-free.'}, {'question': 'What is the expected trend in LFP type battery production?', 'answer': 'The expected trend in LFP type battery production is to rise further and surpass lithium nickel manganese cobalt oxides (NMC) type batteries in 2028.'}]}\n",
    "        return qna\n",
    "        \n",
    "def process_chunk(context:str,llmmsg:str)->SeedExample:\n",
    "    it = llm_base.complete(llmmsg,max_tokens=MAX_TOKENS_QNA,timeout=120.0)\n",
    "        # Ensure that we just take the json output, sometimes we get some rubbish upfront\n",
    "    json_start = it.text.find('{')\n",
    "\n",
    "    extracted_json = extract_valid_json(it)\n",
    "    print(extracted_json)\n",
    "    if not extracted_json:\n",
    "        print(\"Could not extract valid JSON.\")\n",
    "        return None\n",
    "\n",
    "    # Filter malformed Q&A entries\n",
    "    extracted_json[\"questions_and_answers\"] = [\n",
    "        qa for qa in extracted_json.get(\"questions_and_answers\", [])\n",
    "        if isinstance(qa, dict) and qa.get(\"question\") and qa.get(\"answer\")\n",
    "    ]\n",
    "\n",
    "    json_str = json.dumps(extracted_json)\n",
    "    res = SeedExampleQNAOnly.model_validate(from_json(json_str,allow_partial=True,cache_strings='keys'))\n",
    "    fin = SeedExample(context=context,questions_and_answers=res.questions_and_answers)\n",
    "\n",
    "    if fin.questions_and_answers is None:\n",
    "        raise Exception(\"Invalid payload, no qna\")\n",
    "    \n",
    "    return fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_prompt=f\"You are a helpful question and answer writing assistant. Given the following Information generate 1 SeedExample containing 3 question and answer pairs. Ensure that the questions can be answered by the information given. Do not number the pairs.  All output MUST be in valid JSON format.\\n\\nInformation:\"\n",
    "\n",
    "json_prompt=f\"\\n\\nOutput a valid JSON object but do not repeat the schema. This is the JSON schema that must be used: {SeedExampleQNAOnly.model_json_schema()}.\"\n",
    "result_output=\"\"\n",
    "seed_examples=[]\n",
    "\n",
    "clen = len(chunks[0])\n",
    "\n",
    "for idx,ch in enumerate(chunks[0]):\n",
    "    _log.info(f\"Chunk {idx} of {clen}\")\n",
    "    llm_msg = gen_prompt+ch.content+json_prompt\n",
    "    _log.debug(llm_msg)\n",
    "\n",
    "    valid_output = False\n",
    "    retry_count = 0\n",
    "\n",
    "    while not valid_output and retry_count < 3:\n",
    "        try:\n",
    "            seed_examples.append(process_chunk(ch.content,llm_msg))\n",
    "        except (Exception) as e:\n",
    "            _log.error(e,f\"Chunk {idx} -> Invalid response,count {retry_count}\")\n",
    "            retry_count += 1\n",
    "        else:\n",
    "            valid_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalqna = QNAModel(version=3,created_by=\"noelo\",domain=\"Batteries\",seed_examples=seed_examples)\n",
    "jsonout = finalqna.model_dump_json()\n",
    "\n",
    "import json\n",
    "python_dict=json.loads(jsonout)\n",
    "yaml_string=yaml.dump(python_dict)\n",
    "\n",
    "# outputyaml=to_yaml_str(finalqna)\n",
    "with open('qna.yaml', 'w') as file:\n",
    "    file.write(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "\n",
    "def upload_file_to_minio(file_path, bucket_name, object_name=None, endpoint_url='http://minio.summit-project.svc.cluster.local:9000', access_key='minio', secret_key='minio123'):\n",
    "    \"\"\"Upload a file to an S3 bucket.\n",
    "\n",
    "    Args:\n",
    "        file_path: File to upload.\n",
    "        bucket_name: Bucket to upload to.\n",
    "        object_name: S3 object name. If not specified then file_path is used.\n",
    "        endpoint_url: MinIO endpoint URL.\n",
    "        access_key: MinIO access key.\n",
    "        secret_key: MinIO secret key.\n",
    "\n",
    "    Returns:\n",
    "        True if file was uploaded, else False.\n",
    "    \"\"\"\n",
    "\n",
    "    # If S3 object_name was not specified, use file_path\n",
    "    if object_name is None:\n",
    "        object_name = os.path.basename(file_path)\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3',\n",
    "                              endpoint_url=endpoint_url,\n",
    "                              aws_access_key_id=access_key,\n",
    "                              aws_secret_access_key=secret_key)\n",
    "    try:\n",
    "        response = s3_client.upload_file(file_path, bucket_name, object_name)\n",
    "        print(f\"File '{file_path}' uploaded to '{bucket_name}/{object_name}'\")\n",
    "        return True\n",
    "    except ClientError as e:\n",
    "        print(f\"Error uploading file: {e}\")\n",
    "        return False\n",
    "    except FileNotFoundError:\n",
    "      print(f\"Error: File '{file_path}' not found.\")\n",
    "      return False\n",
    "\n",
    "# Example usage (replace with your actual values):\n",
    "file_path = 'qna.yaml'  # Replace with the path to your file\n",
    "bucket_name = 'data-files-bucket' # Replace with your bucket name\n",
    "\n",
    "#create example file if it doesn't exist.\n",
    "if not os.path.exists(file_path):\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(\"This is an example file.\")\n",
    "\n",
    "if upload_file_to_minio(file_path, bucket_name):\n",
    "    print(\"Upload of PDF file successful! Data Science Pipeline should be starting.\")\n",
    "else:\n",
    "    print(\"Upload failed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
